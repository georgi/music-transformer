_target_: src.models.transformer_model.TransformerModel

architecture: llama
lr: 0.1
warmup_steps: 2000
betas: [0.9, 0.95]
weight_decay: 0.1
n_positions: ${max_seq}
vocab_size: ${vocab_size}
n_embed: 512
n_head: 16
n_layer: 8